# Chapter02 데이터

## 02-1 0과 1로 숫자를 표현하는 방법

- 3+4 를 컴퓨터가 인식할까?
### 정보 단위

  - bit : 0 or 1
    - (꺼짐), (켜짐)
      - 1bit -> 2개 표현 가능
      - 2bit -> 4개 표현 가능
      - 3bit -> 8개 표현 가능
      - 4bit -> 16개 표현 가능
      - nbit -> 2^n 개 표현 가능

---------------------------------
    - 1byte : 8bit
    - 1KB = 1000byte
    - 1MB = 1000KB
    - 1GB = 1000MB
    - 1TB = 1000GB
----------------------------------

  - **1워드**
    - **CPU가 한 번에 처리할 수 있는 데이터 크기**
    - x86 CPU -> 32bit 워드
    - x64 CPU -> 64bit 워드


- 이진법
  - 0과 1로 숫자를 표현하는 방법
  - 앞에 `0b` 를 붙여줌 or `(2)` 를 써줌


  - 어떻게 2진법으로 음수 표현 하는가?
    - 2의 보수 -> 정의 : `어떤 수를 그보다 큰 2^n 을 구해 뺀 값`  
      - 음의 연산을 두번하면 자기 자신 => 보수의 연산을 두번 하면 자기 자신
      - `모든 0과 1을 뒤집고, 거기에 1을 더한 값`
    - ex)
      - 11 의 보수 ? 
        - 뒤집으면 : 00 
        - 1 더하면 -> `01`

    - 플래그를 써서 음수임을 표시함.



- 십육진법
  - 앞에 `ox` 를 붙여줌
  - 123456789ABCDEF 10 11 12 ...




- 십육진수 -> 이진수
  - 각 자리를 2진수로 적어서 이어 붙이면 됨.
  - `1A2B(16)`
  - `0001(2)` + `1010(2)` + `0010(2)` + `1011(2)`
  - `0001101000101011(2)`

- 이진수 -> 십육진수
  - 네자리 별로 16진수로 만들고 연결해주면 됨.


    







# 02-2 0과 1로 문자를 표현하는 방법

컴퓨터는 어떻게 문자를 이해하는가 ? 

  - **문자 집합**
    - 컴퓨터가 인식할 수 있는 문자의 모음으로, 속하지 못한 문자는 인식 못한다.
    - 문자 집합에 속한 문자를 인코딩하여 0과 1로 표현할 수 있음.
  - 문자 인코딩
    - 문자를 숫자 체계로 변환하는 것
  - 문자 디코딩
    - 0과 1로 표현된 문자를 원래의 문자로 변환 하는 것
  
##
  - 아스키 코드  
    - 2^7 -> 128개 문자 표현
    - 턱없이 부족함.


  - EUC-KR 은 한글을 2바이트 크기로 인코딩할 수 있는 완성형 인코딩 방식
    - 완성형 인코딩 ?
      - 하나의 글자에 고유한 코드 부여
    - 조합형 인코딩 ? 
      - 초, 중, 종 성을 위한 비트열 별도 할당
    - EUC-KR 은 완성형 인코딩 ,, 따라서 뷁 <- 표현못함


  - 유니코드
    - 여러 나라의 문자들을 광범위하게 표현할 수 있는 통일된 문자 집합
    - 현대적 방식
    - 바이트별 인코딩 결과
    - UTF-8, UTF-16, UTF-32
    - 한국말 -> 3바이트